%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}


\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\usepackage{graphicx}
\usepackage{float}
\usepackage{placeins}
\usepackage{subcaption}
\usepackage{cleveref}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\newcommand{\acomment}[1]{{\bf{\color{blue}{{[Aman: #1]}}}}}
\newcommand{\bigo}[1]{\ensuremath{\mathcal{O} (#1)}}
\newcommand{\fullref}[1]{\hyperref[{#1}]{\autoref*{#1} - \nameref*{#1}}} % One single link
\newcommand{\figref}[1]{\hyperref[{#1}]{\autoref*{#1}}} % One single link


\title{Automated Essay Scoring}

\author{Aman Agrawal \\
  IIT Delhi \\
  {\tt cs1150210@iitd.ac.in} \\\And
  Suyash Agrawal \\
  IIT Delhi \\
  {\tt cs1150262@iitd.ac.in} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Automated essay scoring systems(AES) are used in evaluating and scoring student essays written based on a given prompt. Recent advances in Deep Learning and Natural Language Processing have produced state of art results in this task. These systems are approaching human level performance according to the evaluation metrics, but their qualitative performance in actual scenarios is not yet explored. In this paper we explore and analyze how these systems perform in real life scenarios as compared to other non-neural models. We also explore some ways to overcome these shortcomings.
\end{abstract}

\section{Introduction}

Essay writing is usually a part of the student assessment  process. Several  organizations, such as Educational Testing Service (ETS)\cite{ets}, evaluate the writing skills of students in their examinations. Because of the large number of students participating in these exams, grading all essays is very time-consuming. 
Thus, automated essay scoring systems(AES) are used in evaluating and scoring student essays written based on a given prompt. The performance of these systems is assessed by comparing their scores assigned to a set of essays to human-assigned gold-standard scores.
Since the output of AES systems is usually a real valued number, the task is often addressed as a supervised machine learning task.
Recently, there is a lot of development of deep neural models for the task of automated essay scoring.
These models have shown to out perform the traditional models based on hand crafted features and currently have the state of art performance in this task. The current state of art model that we presently know of is from Taghipour et al. \cite{nea} and the state of art non-neural model is Enhanced AI Scoring Engine (EASE) \cite{ease}.
In this paper we explore the performance of deep-learning based systems on real life scenarios. We try to see if their performance is actually better than traditional non-neural based models in a qualitative sense and analyze the issues that these systems can cause.

  % The rest of this paper is organized as follows. Section 2 gives an overview of related work in the literature. Section 3 describes the  details of our experiments, and present and discuss the results of our experimental evaluation in Section 4. Finally, we suggest some ways to improve the models in Section 5 and conclude the paper in Section 6.


\section{Related Work}

There exist many automated essay scoring systems \cite{shermis2013handbook} and some of them are being used in high-stakes assessments. E-rater \cite{attali2004automated} and Intelligent Essay Assessor \cite{foltz1999intelligent} are two notable examples of AES systems. In 2012, a competition on automated essay scoring called \textit{Automated Student Assessment Prize (ASAP)} was organized by Kaggle and sponsored by the Hewlett Foundation. A comprehensive comparison of AES systems was made in the ASAP competition. Many AES systems have been developed to date, and most of them have been built with hand-crafted features and supervised machine learning algorithms. Researchers have devoted a substantial amount of effort to design effective features for automated essay scoring. These features can be as simple as essay length \cite{chen2013automated} or more complicated such as lexical complexity, grammaticality of a text \cite{attali2004automated}, or syntactic features \cite{chen2013automated}. Readability features \cite{zesch2015task} have also been proposed in the literature as another source of information. Moreover, text coherence has also been exploited to assess the flow of information and argumentation of an essay \cite{chen2013automated}. A detailed overview of the features used in AES systems can be found in \cite{zesch2015task}. 

Recently due to a lot of surge in deep-learning, a lot of neural models have been proposed which claim to beat the human level performance on evaluation metrics. Unlike traditional systems these systems, accepts an essay text as input directly and learns the features automatically from the data. A lot of models with varying architectures have been developed in the recent years. Some of them use a long short-term memory network (LSTM) \cite{taghipour2016neural}, while others use a convolutional neural networks (CNN) \cite{dong2016automatic} for the effect of automatically learning features. Some of them also use an augmented method to learn word embeddings \cite{alikaniotis2016automatic}. 

We have tried to analyze these models over various qualitative metrics on which a human scorer would have graded the essays. We consider the length of the essay, grammatical and spelling mistakes, and the flow of ideas and organization of essay. Along with this, we also try to test if any of these models recognize deviation from the specifed topic. We finally suggest some ways to improve the performance of model on these qualitative metrics.



\section{Model}\label{sec:models}

\subsection{Neural Model}

The present state of the art neural model is from \cite{taghipour2016neural}. They use a simple a LSTM based model, where the output of each LSTM unit is averaged over time, to get a representation of the essay. Finally, they use a linear layer with sigmoid activation which maps its input vector generated by the mean-over-time layer to a scalar value and limits the possible scores to the range of $(0, 1)$.

\subsection{Non-Neural Model}

The state of the art, open-source non neural model is Enhanced AI Scoring Engine (EASE) \cite{ease}. Unlike, the neural model, this model uses a lot of hand-crafted features to learn a vector representation of the input essay. These hand-crafted features encode the essay length, spelling mistakes, grammatical correctness and some of content-based features. Finally, a Gradient Boosting Regressor is used to predict the score of the essay.


\section{Experiments}

\subsection{Setup}
The dataset that we have used in our experiments is the same dataset used in the ASAP competition run by Kaggle. We use quadratic weighted Kappa as a quantitative evaluation metric. We trained both the state of the art models described in \cref{sec:models} using the code released by the authors. Same hyper-parameters were used as reported in respective papers in order to reproduce the results stated in the papers.

The dataset contains essay from eight different prompts. Since the test data is not publicly available, the model was trained on 64\% of the data and tested on the remaining 20\%. The remaining 16\% was used to tune hyper-parameters and select the best model.
All of our analysis stated below has been done using the test dataset.

\subsection{Effect of Length}

We tried to analyze the effect of the length of the essay on the prediction of the models. Typically, low scoring essays have low scores and this can be seen from \cref{fig:length_plot}. Let, $\mu_l$ denote the mean length of the essays in the test set and $\sigma_l$ denote the standard deviation of the lengths. We selected essays from each prompt that had length less than $\mu_l - 2\sigma_l$ and appended the same essay till the point where the length of the essay exceeded $\mu_l - 2\sigma_l$. The mean score of these essays before and after increasing length were calculated and the percentage increase in the score was calculated. \ref{length} shows that the average increase in the predicted score of the essay is $100.58\%$ while for some test sets the increase in the score was close to $235.53\%$. 

\begin{figure*}
  \centering             
  \begin{subfigure}[ht]{0.475\textwidth}
      \centering
      \includegraphics[width=\textwidth,ext=pdf]{{../../plots/score_vs_len_4_test}.pdf}
  \end{subfigure}
  \quad
  \begin{subfigure}[ht]{0.475\textwidth}
      \centering
      \includegraphics[width=\textwidth,ext=pdf]{{../../plots/score_vs_len_8_test}.pdf}
  \end{subfigure}
  \caption{Distribution of Length of essay and associated score in the test dataset \label{fig:length_plot}}        
\end{figure*}


\begin{table}[]
  \centering
  \resizebox{0.5\textwidth}{!}{%
  \begin{tabular}{|c|c|c|c|}
  \hline
  \textbf{Prompt} & \textbf{Essays Tested} & \textbf{\%increase (Neural)} & \textbf{\%increase (Non-Neural)} \\ \hline
  1 & 37 & \textbf{72.785} & \textbf{37.240} \\ \hline
  2 & 45 & 53.428 & \textbf{33.843} \\ \hline
  3 & 93 & \textbf{137.284} & \textbf{56.061} \\ \hline
  4 & 85 & \textbf{235.539} & \textbf{44.515} \\ \hline
  5 & 70 & \textbf{145.933} & \textbf{78.571} \\ \hline
  6 & 72 & \textbf{119.254} & 161.905 \\ \hline
  7 & 53 & 40.181 & \textbf{14.659} \\ \hline
  8 & 23 & 0.279 & \textbf{4.141} \\ \hline
  \end{tabular}%
  }
\caption{Effect of Length}
\label{length}
\end{table}

\subsection{Effect of Spelling Mistakes}
We tried to analyze the effect of the spelling mistakes on the prediction of the models. We randomly chose 20\% of the words of the essay and then randomly shuffled the characters of the words to generate words with wrong spellings. We had a set of predicted scores for the essays with correct spellings and a new set of predicted scores for essays with wrong spellings. We calculated QWK between these two sets of scores. \cref{spelling_word} shows that for the neural model, the QWK is close to 1 for most prompts whereas it is very close to 0 for non-neural model. Thus, for the neural model there is no disagreement between the scores predicted for essays with correct spellings and essays with 20\% wrong spellings.

\subsection{Effect of Word Shuffle}
We tried to analyze the effect of the spelling mistakes on the prediction of the models. We randomly shuffled the words of each sentence in the essay. This resulted in an essay with terrible grammar, and thus should be scored low by any good AES system. We had a set of predicted scores for the essays with good grammar and a new set of predicted scores for essays with bad grammar and a lot of grammatical mistakes. We calculated QWK between these two sets of scores. \cref{spelling_word} shows that for the neural model, the QWK is close to 1 for most prompts whereas it is significantly less than 1 for the non-neural model. Thus, for the neural model there is no disagreement between the scores predicted for essays with correct grammar and essays with bad grammar.

\begin{table*}[h]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{|c|c|c|c|c|c|}
  \hline
  \textbf{Prompt} & \textbf{Essays Tested} & \textbf{QWK Spelling (Neural)} & \textbf{QWK Spelling (Non-Neural)} & \textbf{QWK Word Shuffle (Neural)} & \textbf{QWK Word Shuffle (Non- Neural)} \\ \hline
  1 & 357 & \textbf{0.944} & \textbf{0.027} & \textbf{0.940} & 0.991 \\ \hline
  2 & 360 & 0.408 & \textbf{0.013} & 0.739 & \textbf{0.515} \\ \hline
  3 & 345 & \textbf{0.901} & \textbf{0.163} & \textbf{0.893} & 0.709 \\ \hline
  4 & 355 & \textbf{0.936} & \textbf{0.019} & \textbf{0.930} & \textbf{0.495} \\ \hline
  5 & 361 & \textbf{0.930} & \textbf{0.027} & \textbf{0.913} & \textbf{0.251} \\ \hline
  6 & 360 & 0.591 & \textbf{0.017} & \textbf{0.914} & \textbf{0.306} \\ \hline
  7 & 314 & \textbf{0.871} & \textbf{0.081} & \textbf{0.944} & 0.962 \\ \hline
  8 & 145 & \textbf{0.822} & \textbf{0.086} & \textbf{0.934} & 0.931 \\ \hline
  \end{tabular}%
  }
  \caption{Effect of Spelling Mistakes and Word Shuffle}
  \label{spelling_word}
  \end{table*}

\FloatBarrier

\bibliography{paper}
\bibliographystyle{acl_natbib}

\end{document}
